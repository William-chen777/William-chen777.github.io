<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.0.3" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/bear.png?v=6.0.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/bear.png?v=6.0.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/bear.png?v=6.0.3">










<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.0.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  
  <meta name="keywords" content="数据处理,">


<meta name="description" content="数据预处理在sklearn当中，我们使用preprocessing.MinMaxScaler来实现这个功能。MinMaxScaler有一个重要参数，feature_range，控制我们希望把数据压缩到的范围，默认是[0,1]。">
<meta name="keywords" content="数据处理">
<meta property="og:type" content="article">
<meta property="og:title" content="数据预处理和特征工程">
<meta property="og:url" content="https://william-chen777.github.io/2020/08/02/数据预处理和特征工程/index.html">
<meta property="og:site_name" content="William Chen">
<meta property="og:description" content="数据预处理在sklearn当中，我们使用preprocessing.MinMaxScaler来实现这个功能。MinMaxScaler有一个重要参数，feature_range，控制我们希望把数据压缩到的范围，默认是[0,1]。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200802150933620.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTA0NDc1OA==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200802151015843.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTA0NDc1OA==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200802151030936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTA0NDc1OA==,size_16,color_FFFFFF,t_70">
<meta property="og:updated_time" content="2020-08-02T07:31:14.475Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="数据预处理和特征工程">
<meta name="twitter:description" content="数据预处理在sklearn当中，我们使用preprocessing.MinMaxScaler来实现这个功能。MinMaxScaler有一个重要参数，feature_range，控制我们希望把数据压缩到的范围，默认是[0,1]。">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20200802150933620.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTA0NDc1OA==,size_16,color_FFFFFF,t_70">






  <link rel="canonical" href="https://william-chen777.github.io/2020/08/02/数据预处理和特征工程/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>
  <title>数据预处理和特征工程 | William Chen</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">William Chen</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>
        </li>
      

      
    </ul>
  

  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://william-chen777.github.io/2020/08/02/数据预处理和特征工程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chen kun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="William Chen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">数据预处理和特征工程</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-08-02T15:08:44+08:00">2020-08-02</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/sklearn/" itemprop="url" rel="index"><span itemprop="name">sklearn</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>在sklearn当中，我们使用preprocessing.MinMaxScaler来实现这个功能。MinMaxScaler有一个重要参数，feature_range，控制我们希望把数据压缩到的范围，默认是[0,1]。</p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = [[<span class="number">-1</span>,<span class="number">2</span>], [<span class="number">-0.5</span>,<span class="number">6</span>], [<span class="number">0</span>,<span class="number">10</span>], [<span class="number">1</span>,<span class="number">18</span>]]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(data)</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1.0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.5</td>
      <td>6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>10</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.0</td>
      <td>18</td>
    </tr>
  </tbody>
</table>
</div>



<h2 id="实现归一化"><a href="#实现归一化" class="headerlink" title="实现归一化"></a>实现归一化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scaler = MinMaxScaler()                 <span class="comment">#实例化</span></span><br><span class="line">scaler = scaler.fit(data)               <span class="comment">#fit, 在这里的本质是生成min(x) 和 max(x)</span></span><br><span class="line">result = scaler.transform(data)         <span class="comment">#导出结果</span></span><br><span class="line"></span><br><span class="line">result</span><br></pre></td></tr></table></figure>

<pre><code>array([[0.  , 0.  ],
       [0.25, 0.25],
       [0.5 , 0.5 ],
       [1.  , 1.  ]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result_ = scaler.fit_transform(data)      <span class="comment">#训练和导出结果一步达成</span></span><br><span class="line"></span><br><span class="line">result_</span><br></pre></td></tr></table></figure>

<pre><code>array([[0.  , 0.  ],
       [0.25, 0.25],
       [0.5 , 0.5 ],
       [1.  , 1.  ]])</code></pre><p><strong>数据分布一致</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scaler.inverse_transform(result)         <span class="comment">#将归一化之后的结果逆转</span></span><br></pre></td></tr></table></figure>

<pre><code>array([[-1. ,  2. ],
       [-0.5,  6. ],
       [ 0. , 10. ],
       [ 1. , 18. ]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用MinMaxScaler的参数feature_range实现将数据归一化到[0,1]以外的范围中</span></span><br><span class="line">data = [[<span class="number">-1</span>, <span class="number">2</span>], [<span class="number">-0.5</span>, <span class="number">6</span>], [<span class="number">0</span>, <span class="number">10</span>], [<span class="number">1</span>, <span class="number">18</span>]]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scaler = MinMaxScaler(feature_range=[<span class="number">5</span>,<span class="number">10</span>])  <span class="comment">#实例化</span></span><br><span class="line">result = scaler.fit_transform(data) <span class="comment">#fit_transform一步导出结果</span></span><br><span class="line"></span><br><span class="line">result</span><br></pre></td></tr></table></figure>

<pre><code>array([[ 5.  ,  5.  ],
       [ 6.25,  6.25],
       [ 7.5 ,  7.5 ],
       [10.  , 10.  ]])</code></pre><p><strong>当X中的特征数量非常多的时候，fit会报错并表示，数据量太大了我计算不了<br>此时使用partial_fit作为训练接口<br>scaler = scaler.partial_fit(data)</strong></p>
<h2 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">data = [[<span class="number">-1</span>, <span class="number">2</span>], [<span class="number">-0.5</span>, <span class="number">6</span>], [<span class="number">0</span>, <span class="number">10</span>], [<span class="number">1</span>, <span class="number">18</span>]]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scaler = StandardScaler() <span class="comment">#实例化</span></span><br><span class="line">scaler.fit(data) <span class="comment">#fit，本质是生成均值和方差</span></span><br></pre></td></tr></table></figure>

<pre><code>StandardScaler()</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scaler.mean_ <span class="comment">#查看均值的属性mean_</span></span><br></pre></td></tr></table></figure>

<pre><code>array([-0.125,  9.   ])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scaler.var_ <span class="comment">#查看方差的属性var_</span></span><br></pre></td></tr></table></figure>

<pre><code>array([ 0.546875, 35.      ])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_std = scaler.transform(data) <span class="comment">#通过接口导出结果</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(x_std)</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1.183216</td>
      <td>-1.183216</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.507093</td>
      <td>-0.507093</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.169031</td>
      <td>0.169031</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.521278</td>
      <td>1.521278</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_std.mean() <span class="comment">#导出的结果是一个数组，用mean()查看均值</span></span><br></pre></td></tr></table></figure>

<pre><code>0.0</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_std.std() <span class="comment">#用std()查看方差</span></span><br></pre></td></tr></table></figure>

<pre><code>1.0</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scaler.fit_transform(data) <span class="comment">#使用fit_transform(data)一步达成结果</span></span><br></pre></td></tr></table></figure>

<pre><code>array([[-1.18321596, -1.18321596],
       [-0.50709255, -0.50709255],
       [ 0.16903085,  0.16903085],
       [ 1.52127766,  1.52127766]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scaler.inverse_transform(x_std) <span class="comment">#使用inverse_transform逆转标准化</span></span><br></pre></td></tr></table></figure>

<pre><code>array([[-1. ,  2. ],
       [-0.5,  6. ],
       [ 0. , 10. ],
       [ 1. , 18. ]])</code></pre><h2 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">r"E:\machineLearning\cai\03数据预处理和特征工程\Narrativedata.csv"</span>,index_col=<span class="number">0</span>)   <span class="comment">#index_col=0是把第0列做索引，否则会把第一列作为特征</span></span><br><span class="line"></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Sex</th>
      <th>Embarked</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22.0</td>
      <td>male</td>
      <td>S</td>
      <td>No</td>
    </tr>
    <tr>
      <th>1</th>
      <td>38.0</td>
      <td>female</td>
      <td>C</td>
      <td>Yes</td>
    </tr>
    <tr>
      <th>2</th>
      <td>26.0</td>
      <td>female</td>
      <td>S</td>
      <td>Yes</td>
    </tr>
    <tr>
      <th>3</th>
      <td>35.0</td>
      <td>female</td>
      <td>S</td>
      <td>Yes</td>
    </tr>
    <tr>
      <th>4</th>
      <td>35.0</td>
      <td>male</td>
      <td>S</td>
      <td>No</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.info</span><br></pre></td></tr></table></figure>

<pre><code>&lt;bound method DataFrame.info of       Age     Sex Embarked Survived
0    22.0    male        S       No
1    38.0  female        C      Yes
2    26.0  female        S      Yes
3    35.0  female        S      Yes
4    35.0    male        S       No
..    ...     ...      ...      ...
886  27.0    male        S       No
887  19.0  female        S      Yes
888   NaN  female        S       No
889  26.0    male        C  Unknown
890  32.0    male        Q       No

[891 rows x 4 columns]&gt;</code></pre><h3 id="填补年龄"><a href="#填补年龄" class="headerlink" title="填补年龄"></a>填补年龄</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Age = data.loc[:,<span class="string">"Age"</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>) <span class="comment">#sklearn当中特征矩阵必须是二维(data.loc 是按索引来取所有的行)（.values是取出所有值，不需要索引）（因为必须是二维，所以用reshape重构）</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">imp_mean = SimpleImputer() <span class="comment">#实例化，默认均值填补</span></span><br><span class="line">imp_median = SimpleImputer(strategy=<span class="string">"median"</span>) <span class="comment">#用中位数填补</span></span><br><span class="line">imp_0 = SimpleImputer(strategy=<span class="string">"constant"</span>,fill_value=<span class="number">0</span>) <span class="comment">#用0填补</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">imp_mean = imp_mean.fit_transform(Age) <span class="comment">#fit_transform一步完成调取结果</span></span><br><span class="line">imp_median = imp_median.fit_transform(Age)</span><br><span class="line">imp_0 = imp_0.fit_transform(Age)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">imp_mean[:<span class="number">20</span>]</span><br></pre></td></tr></table></figure>

<pre><code>array([[22.        ],
       [38.        ],
       [26.        ],
       ......
       [29.69911765],
       [31.        ],
       [29.69911765]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">imp_median[:<span class="number">20</span>]</span><br></pre></td></tr></table></figure>

<pre><code>array([[22.],
       [38.],
       [26.],
       [35.],
       [35.],
     .......
       [31.],
       [28.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">imp_0[:<span class="number">20</span>]</span><br></pre></td></tr></table></figure>

<pre><code>array([[22.],
       [38.],
       [26.],
       [35.],
       [35.],
       [ 0.],
       .....
       [31.],
       [ 0.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在这里我们使用中位数填补Age</span></span><br><span class="line">data.loc[:,<span class="string">"Age"</span>] = imp_median</span><br><span class="line">data.info()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
Int64Index: 891 entries, 0 to 890
Data columns (total 4 columns):
 #   Column    Non-Null Count  Dtype  
---  ------    --------------  -----  
 0   Age       891 non-null    float64
 1   Sex       891 non-null    object 
 2   Embarked  889 non-null    object 
 3   Survived  891 non-null    object 
dtypes: float64(1), object(3)
memory usage: 34.8+ KB</code></pre><h3 id="使用众数填补Embarked"><a href="#使用众数填补Embarked" class="headerlink" title="使用众数填补Embarked"></a>使用众数填补Embarked</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Embarked = data.loc[:,<span class="string">"Embarked"</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">imp_mode = SimpleImputer(strategy = <span class="string">"most_frequent"</span>)</span><br><span class="line">data.loc[:,<span class="string">"Embarked"</span>] = imp_mode.fit_transform(Embarked)</span><br><span class="line">data.info()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
Int64Index: 891 entries, 0 to 890
Data columns (total 4 columns):
 #   Column    Non-Null Count  Dtype  
---  ------    --------------  -----  
 0   Age       891 non-null    float64
 1   Sex       891 non-null    object 
 2   Embarked  891 non-null    object 
 3   Survived  891 non-null    object 
dtypes: float64(1), object(3)
memory usage: 34.8+ KB</code></pre><h3 id="用Pandas和Numpy进行填补"><a href="#用Pandas和Numpy进行填补" class="headerlink" title="用Pandas和Numpy进行填补"></a>用Pandas和Numpy进行填补</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data_ = pd.read_csv(<span class="string">r"E:\machineLearning\cai\03数据预处理和特征工程\Narrativedata.csv"</span>,index_col=<span class="number">0</span>)</span><br><span class="line">data_.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Sex</th>
      <th>Embarked</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22.0</td>
      <td>male</td>
      <td>S</td>
      <td>No</td>
    </tr>
    <tr>
      <th>1</th>
      <td>38.0</td>
      <td>female</td>
      <td>C</td>
      <td>Yes</td>
    </tr>
    <tr>
      <th>2</th>
      <td>26.0</td>
      <td>female</td>
      <td>S</td>
      <td>Yes</td>
    </tr>
    <tr>
      <th>3</th>
      <td>35.0</td>
      <td>female</td>
      <td>S</td>
      <td>Yes</td>
    </tr>
    <tr>
      <th>4</th>
      <td>35.0</td>
      <td>male</td>
      <td>S</td>
      <td>No</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_.loc[:,<span class="string">"Age"</span>] = data_.loc[:,<span class="string">"Age"</span>].fillna(data_.loc[:,<span class="string">"Age"</span>].median())</span><br><span class="line"><span class="comment">#.fillna 在DataFrame里面直接进行填补</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_.dropna(axis=<span class="number">0</span>,inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#.dropna(axis=0)删除所有有缺失值的行，.dropna(axis=1)删除所有有缺失值的列</span></span><br><span class="line"><span class="comment">#参数inplace，为True表示在原数据集上进行修改，为False表示生成一个复制对象，不修改原数据，默认False</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_.info</span><br></pre></td></tr></table></figure>

<pre><code>&lt;bound method DataFrame.info of       Age     Sex Embarked Survived
0    22.0    male        S       No
1    38.0  female        C      Yes
2    26.0  female        S      Yes
3    35.0  female        S      Yes
4    35.0    male        S       No
..    ...     ...      ...      ...
886  27.0    male        S       No
887  19.0  female        S      Yes
888  28.0  female        S       No
889  26.0    male        C  Unknown
890  32.0    male        Q       No

[889 rows x 4 columns]&gt;</code></pre><h2 id="处理分类型特征：编码和哑变量"><a href="#处理分类型特征：编码和哑变量" class="headerlink" title="处理分类型特征：编码和哑变量"></a>处理分类型特征：编码和哑变量</h2><h3 id="preprocessing-LabelEncoder：标签专用，能够将分类转换为分类数值"><a href="#preprocessing-LabelEncoder：标签专用，能够将分类转换为分类数值" class="headerlink" title="preprocessing.LabelEncoder：标签专用，能够将分类转换为分类数值"></a>preprocessing.LabelEncoder：标签专用，能够将分类转换为分类数值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">y = data.iloc[:,<span class="number">-1</span>] <span class="comment">#要输入的是标签，不是特征矩阵，所以允许一维      特征矩阵</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">le = LabelEncoder() <span class="comment">#实例化</span></span><br><span class="line">le = le.fit(y) <span class="comment">#导入数据</span></span><br><span class="line">label = le.transform(y) <span class="comment">#transform接口调取结果</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">label</span><br></pre></td></tr></table></figure>

<pre><code>array([0, 2, 2, 2, 0, 0, 0, 0, 2, 2, 1, 2, 0, 0, 0, 1, 0, 2, 0, 2, 1, 2,
       2, 2, 0, 1, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1,
       2, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 0, 2,
      .............................
       0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 1, 0, 1,
       0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 2,
       0, 0, 2, 2, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 2, 0, 2, 2, 2,
       2, 0, 0, 0, 2, 0, 1, 1, 1, 0, 0, 2, 0, 1, 0, 0, 2, 2, 0, 0, 0, 2,
       2, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">le.classes_ <span class="comment">#属性.classes_查看标签中究竟有多少类别</span></span><br></pre></td></tr></table></figure>

<pre><code>array([&apos;No&apos;, &apos;Unknown&apos;, &apos;Yes&apos;], dtype=object)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">le.fit_transform(y) <span class="comment">#也可以直接fit_transform一步到位</span></span><br></pre></td></tr></table></figure>

<pre><code>array([0, 2, 2, 2, 0, 0, 0, 0, 2, 2, 1, 2, 0, 0, 0, 1, 0, 2, 0, 2, 1, 2,
       2, 2, 0, 1, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1,
       2, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 0, 2,
       2, 0, 2, 0, 0, 0, 0, 0, 2, 1, 0, 1, 2, 2, 0, 2, 2, 0, 2, 2, 0, 0,
       ..........
       0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 2,
       0, 0, 2, 2, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 2, 0, 2, 2, 2,
       2, 0, 0, 0, 2, 0, 1, 1, 1, 0, 0, 2, 0, 1, 0, 0, 2, 2, 0, 0, 0, 2,
       2, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">le.inverse_transform(label) <span class="comment">#使用inverse_transform可以逆转</span></span><br></pre></td></tr></table></figure>

<pre><code>array([&apos;No&apos;, &apos;Yes&apos;, &apos;Yes&apos;, &apos;Yes&apos;, &apos;No&apos;, &apos;No&apos;, &apos;No&apos;, &apos;No&apos;, &apos;Yes&apos;, &apos;Yes&apos;,
       &apos;Unknown&apos;, &apos;Yes&apos;, &apos;No&apos;, &apos;No&apos;, &apos;No&apos;, &apos;Unknown&apos;, &apos;No&apos;, &apos;Yes&apos;, &apos;No&apos;,
       &apos;Yes&apos;, &apos;Unknown&apos;, &apos;Yes&apos;, &apos;Yes&apos;, &apos;Yes&apos;, &apos;No&apos;, &apos;Unknown&apos;, &apos;No&apos;, &apos;No&apos;,
       ..........
       &apos;No&apos;, &apos;No&apos;, &apos;Unknown&apos;, &apos;Unknown&apos;, &apos;No&apos;, &apos;Unknown&apos;, &apos;Yes&apos;, &apos;No&apos;,
       &apos;Yes&apos;, &apos;Yes&apos;, &apos;Yes&apos;, &apos;Yes&apos;, &apos;No&apos;, &apos;No&apos;, &apos;No&apos;, &apos;Yes&apos;, &apos;No&apos;,
       &apos;Unknown&apos;, &apos;Unknown&apos;, &apos;Unknown&apos;, &apos;No&apos;, &apos;No&apos;, &apos;Yes&apos;, &apos;No&apos;,
       &apos;Unknown&apos;, &apos;No&apos;, &apos;No&apos;, &apos;Yes&apos;, &apos;Yes&apos;, &apos;No&apos;, &apos;No&apos;, &apos;No&apos;, &apos;Yes&apos;,
       &apos;Yes&apos;, &apos;No&apos;, &apos;No&apos;, &apos;Unknown&apos;, &apos;No&apos;, &apos;No&apos;, &apos;No&apos;, &apos;Yes&apos;, &apos;No&apos;,
       &apos;Unknown&apos;, &apos;No&apos;], dtype=object)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.iloc[:,<span class="number">-1</span>] = label <span class="comment">#让标签等于我们运行出来的结果</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Sex</th>
      <th>Embarked</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22.0</td>
      <td>male</td>
      <td>S</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>38.0</td>
      <td>female</td>
      <td>C</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>26.0</td>
      <td>female</td>
      <td>S</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>35.0</td>
      <td>female</td>
      <td>S</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>35.0</td>
      <td>male</td>
      <td>S</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#from sklearn.preprocessing import LabelEncoder</span></span><br><span class="line"><span class="comment">#data.iloc[:,-1] = LabelEncoder().fit_transform(data.iloc[:,-1])</span></span><br></pre></td></tr></table></figure>

<h3 id="preprocessing-OrdinalEncoder：特征专用，能够将分类特征转换为分类数值"><a href="#preprocessing-OrdinalEncoder：特征专用，能够将分类特征转换为分类数值" class="headerlink" title="preprocessing.OrdinalEncoder：特征专用，能够将分类特征转换为分类数值"></a>preprocessing.OrdinalEncoder：特征专用，能够将分类特征转换为分类数值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OrdinalEncoder</span><br><span class="line"><span class="comment">#接口categories_对应LabelEncoder的接口classes_，一模一样的功能</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_ = data.copy()</span><br><span class="line">data_.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Sex</th>
      <th>Embarked</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22.0</td>
      <td>male</td>
      <td>S</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>38.0</td>
      <td>female</td>
      <td>C</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>26.0</td>
      <td>female</td>
      <td>S</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>35.0</td>
      <td>female</td>
      <td>S</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>35.0</td>
      <td>male</td>
      <td>S</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OrdinalEncoder().fit(data_.iloc[:,<span class="number">1</span>:<span class="number">-1</span>]).categories_     <span class="comment">#查看特征中有多少类别</span></span><br></pre></td></tr></table></figure>

<pre><code>[array([&apos;female&apos;, &apos;male&apos;], dtype=object), array([&apos;C&apos;, &apos;Q&apos;, &apos;S&apos;], dtype=object)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_.iloc[:,<span class="number">1</span>:<span class="number">-1</span>] = OrdinalEncoder().fit_transform(data_.iloc[:,<span class="number">1</span>:<span class="number">-1</span>])</span><br><span class="line">data_.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Sex</th>
      <th>Embarked</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>38.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>26.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>35.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>35.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



<h3 id="preprocessing-OneHotEncoder：独热编码，创建哑变量"><a href="#preprocessing-OneHotEncoder：独热编码，创建哑变量" class="headerlink" title="preprocessing.OneHotEncoder：独热编码，创建哑变量"></a>preprocessing.OneHotEncoder：独热编码，创建哑变量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Sex</th>
      <th>Embarked</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22.0</td>
      <td>male</td>
      <td>S</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>38.0</td>
      <td>female</td>
      <td>C</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>26.0</td>
      <td>female</td>
      <td>S</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>35.0</td>
      <td>female</td>
      <td>S</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>35.0</td>
      <td>male</td>
      <td>S</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">enc = OneHotEncoder(categories=<span class="string">'auto'</span>).fit(X)</span><br><span class="line">result = enc.transform(X).toarray()</span><br><span class="line">result               <span class="comment">#性别有两个列，舱门有三个列</span></span><br></pre></td></tr></table></figure>

<pre><code>array([[0., 1., 0., 0., 1.],
       [1., 0., 1., 0., 0.],
       [1., 0., 0., 0., 1.],
       ...,
       [1., 0., 0., 0., 1.],
       [0., 1., 1., 0., 0.],
       [0., 1., 0., 1., 0.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#依然可以直接一步到位，但为了给大家展示模型属性，所以还是写成了三步</span></span><br><span class="line">OneHotEncoder(categories=<span class="string">'auto'</span>).fit_transform(X).toarray()</span><br></pre></td></tr></table></figure>

<pre><code>array([[0., 1., 0., 0., 1.],
       [1., 0., 1., 0., 0.],
       [1., 0., 0., 0., 1.],
       ...,
       [1., 0., 0., 0., 1.],
       [0., 1., 1., 0., 0.],
       [0., 1., 0., 1., 0.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#依然可以还原</span></span><br><span class="line">pd.DataFrame(enc.inverse_transform(result))</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>male</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>female</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>female</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>female</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>male</td>
      <td>S</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>886</th>
      <td>male</td>
      <td>S</td>
    </tr>
    <tr>
      <th>887</th>
      <td>female</td>
      <td>S</td>
    </tr>
    <tr>
      <th>888</th>
      <td>female</td>
      <td>S</td>
    </tr>
    <tr>
      <th>889</th>
      <td>male</td>
      <td>C</td>
    </tr>
    <tr>
      <th>890</th>
      <td>male</td>
      <td>Q</td>
    </tr>
  </tbody>
</table>
<p>891 rows × 2 columns</p>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">enc.get_feature_names()           <span class="comment">#查看哑变量的特征类别</span></span><br></pre></td></tr></table></figure>

<pre><code>array([&apos;x0_female&apos;, &apos;x0_male&apos;, &apos;x1_C&apos;, &apos;x1_Q&apos;, &apos;x1_S&apos;], dtype=object)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result</span><br></pre></td></tr></table></figure>

<pre><code>array([[0., 1., 0., 0., 1.],
       [1., 0., 1., 0., 0.],
       [1., 0., 0., 0., 1.],
       ...,
       [1., 0., 0., 0., 1.],
       [0., 1., 1., 0., 0.],
       [0., 1., 0., 1., 0.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result.shape</span><br></pre></td></tr></table></figure>

<pre><code>(891, 5)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#axis=1,表示跨行进行合并，也就是将两表左右相连，如果是axis=0，就是将两表上下相连</span></span><br><span class="line">newdata = pd.concat([data,pd.DataFrame(result)],axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">newdata.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Sex</th>
      <th>Embarked</th>
      <th>Survived</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22.0</td>
      <td>male</td>
      <td>S</td>
      <td>0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>38.0</td>
      <td>female</td>
      <td>C</td>
      <td>2</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>26.0</td>
      <td>female</td>
      <td>S</td>
      <td>2</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>35.0</td>
      <td>female</td>
      <td>S</td>
      <td>2</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>35.0</td>
      <td>male</td>
      <td>S</td>
      <td>0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">newdata.drop([<span class="string">"Sex"</span>,<span class="string">"Embarked"</span>],axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)  <span class="comment">#删除转换完成的特征列</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">newdata.columns =[<span class="string">"Age"</span>,<span class="string">"Survived"</span>,<span class="string">"Female"</span>,<span class="string">"Male"</span>,<span class="string">"Embarked_C"</span>,<span class="string">"Embarked_Q"</span>,<span class="string">"Embarked_S"</span>]   <span class="comment">#将特征名重新命名</span></span><br><span class="line"></span><br><span class="line">newdata.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Survived</th>
      <th>Female</th>
      <th>Male</th>
      <th>Embarked_C</th>
      <th>Embarked_Q</th>
      <th>Embarked_S</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22.0</td>
      <td>0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>38.0</td>
      <td>2</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>26.0</td>
      <td>2</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>35.0</td>
      <td>2</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>35.0</td>
      <td>0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>



<h3 id="处理连续型特征：二值化与分段"><a href="#处理连续型特征：二值化与分段" class="headerlink" title="处理连续型特征：二值化与分段"></a>处理连续型特征：二值化与分段</h3><h4 id="sklearn-preprocessing-Binarizer"><a href="#sklearn-preprocessing-Binarizer" class="headerlink" title="sklearn.preprocessing.Binarizer"></a>sklearn.preprocessing.Binarizer</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将年龄二值化</span></span><br><span class="line">data_2 = data.copy()</span><br><span class="line"></span><br><span class="line">data_2.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Sex</th>
      <th>Embarked</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22.0</td>
      <td>male</td>
      <td>S</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>38.0</td>
      <td>female</td>
      <td>C</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>26.0</td>
      <td>female</td>
      <td>S</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>35.0</td>
      <td>female</td>
      <td>S</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>35.0</td>
      <td>male</td>
      <td>S</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>28.0</td>
      <td>male</td>
      <td>Q</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>54.0</td>
      <td>male</td>
      <td>S</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2.0</td>
      <td>male</td>
      <td>S</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>27.0</td>
      <td>female</td>
      <td>S</td>
      <td>2</td>
    </tr>
    <tr>
      <th>9</th>
      <td>14.0</td>
      <td>female</td>
      <td>C</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Binarizer</span><br><span class="line">X = data_2.iloc[:,<span class="number">0</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>) <span class="comment">#类为特征专用，所以不能使用一维数组</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transformer = Binarizer(threshold=<span class="number">30</span>).fit_transform(X)     <span class="comment"># 将38以上的分为1，一下的分为0</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_2.iloc[:,<span class="number">0</span>] = transformer</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_2.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Sex</th>
      <th>Embarked</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>male</td>
      <td>S</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>female</td>
      <td>C</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>female</td>
      <td>S</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.0</td>
      <td>female</td>
      <td>S</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.0</td>
      <td>male</td>
      <td>S</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.0</td>
      <td>male</td>
      <td>Q</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1.0</td>
      <td>male</td>
      <td>S</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.0</td>
      <td>male</td>
      <td>S</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.0</td>
      <td>female</td>
      <td>S</td>
      <td>2</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.0</td>
      <td>female</td>
      <td>C</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>



<h4 id="preprocessing-KBinsDiscretizer"><a href="#preprocessing-KBinsDiscretizer" class="headerlink" title="preprocessing.KBinsDiscretizer"></a>preprocessing.KBinsDiscretizer</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> KBinsDiscretizer</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = data.iloc[:,<span class="number">0</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">est = KBinsDiscretizer(n_bins=<span class="number">3</span>, encode=<span class="string">'ordinal'</span>, strategy=<span class="string">'uniform'</span>)     <span class="comment">#有几个就几列，等宽分箱</span></span><br><span class="line">est.fit_transform(X)</span><br></pre></td></tr></table></figure>

<pre><code>array([[0.],
       [1.],
       [0.],
       [1.],
     ....
       [0.],
       [1.],
       [0.],
       [1.],
       [0.],
       [1.],
       [1.],
       [0.],
       [1.],
       [0.],
       [1.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看转换后分的箱：变成了一列中的三箱</span></span><br><span class="line">set(est.fit_transform(X).ravel())</span><br></pre></td></tr></table></figure>

<pre><code>{0.0, 1.0, 2.0}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">est = KBinsDiscretizer(n_bins=<span class="number">3</span>, encode=<span class="string">'onehot'</span>, strategy=<span class="string">'uniform'</span>)      <span class="comment">#独热编码，等宽分箱</span></span><br><span class="line"><span class="comment">#查看转换后分的箱：变成了哑变量</span></span><br><span class="line">est.fit_transform(X).toarray()</span><br></pre></td></tr></table></figure>

<pre><code>array([[1., 0., 0.],
       [0., 1., 0.],
       [1., 0., 0.],
       ...,
       [0., 1., 0.],
       [1., 0., 0.],
       [0., 1., 0.]])</code></pre><h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><h2 id="特征工程的第一步是：理解业务"><a href="#特征工程的第一步是：理解业务" class="headerlink" title="特征工程的第一步是：理解业务"></a>特征工程的第一步是：理解业务</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入数据，让我们使用digit recognizor数据来一展身手</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">r"E:\machineLearning\cai\03数据预处理和特征工程\digit recognizor.csv"</span>)</span><br><span class="line"></span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:]       <span class="comment">#特征</span></span><br><span class="line">y = data.iloc[:,<span class="number">0</span>]        <span class="comment">#标签</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.shape</span><br></pre></td></tr></table></figure>

<pre><code>(42000, 784)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>pixel0</th>
      <th>pixel1</th>
      <th>pixel2</th>
      <th>pixel3</th>
      <th>pixel4</th>
      <th>pixel5</th>
      <th>pixel6</th>
      <th>pixel7</th>
      <th>pixel8</th>
      <th>...</th>
      <th>pixel774</th>
      <th>pixel775</th>
      <th>pixel776</th>
      <th>pixel777</th>
      <th>pixel778</th>
      <th>pixel779</th>
      <th>pixel780</th>
      <th>pixel781</th>
      <th>pixel782</th>
      <th>pixel783</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 785 columns</p>
</div>



<h3 id="Filter过滤法"><a href="#Filter过滤法" class="headerlink" title="Filter过滤法"></a>Filter过滤法</h3><p>过滤方法通常用作预处理步骤，特征选择完全独立于任何机器学习算法。它是根据各种统计检验中的分数以及相关<br>性的各项指标来选择特征</p>
<p><strong>全部特征——&gt;最佳特征子集——&gt;算法——&gt;模型评估</strong></p>
<h4 id="方差过滤：VarianceThreshold"><a href="#方差过滤：VarianceThreshold" class="headerlink" title="方差过滤：VarianceThreshold"></a>方差过滤：VarianceThreshold</h4><p>这是通过特征本身的方差来筛选特征的类。比如一个特征本身的方差很小，就表示样本在这个特征上基本没有差异，可能特征中的大多数值都一样，甚至整个特征的取值都相同，那这个特征对于样本区分没有什么作用。<strong>所以无论接下来的特征工程要做什么，都要优先消除方差为0的特征</strong>。VarianceThreshold有重要参数threshold，表示方差的阈值，表示舍弃所有方差小于threshold的特征，不填默认为0，即删除所有的记录都相同的特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">selector = VarianceThreshold() <span class="comment">#实例化，不填参数默认方差为0</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_var0 = selector.fit_transform(X) <span class="comment">#获取删除不合格特征之后的新特征矩阵</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#也可以直接写成 X = VairanceThreshold().fit_transform(X)</span></span><br><span class="line">X_var0.shape</span><br></pre></td></tr></table></figure>

<pre><code>(42000, 708)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(X_var0).head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>698</th>
      <th>699</th>
      <th>700</th>
      <th>701</th>
      <th>702</th>
      <th>703</th>
      <th>704</th>
      <th>705</th>
      <th>706</th>
      <th>707</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 708 columns</p>
</div>



<p>我们已经删除了方差为0的特征，但是依然剩下了708多个特征，明显还需要进一步的特征选择。然而，如果我们知道我们需要多少个特征，方差也可以帮助我们将特征选择一步到位。比如说，我们希望留下一半的特征，那可以设定一个让特征总数减半的方差阈值，只要找到特征方差的中位数，再将这个中位数作为参数<br>threshold的值输入就好了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.var().values  <span class="comment">#查看每一列的方差</span></span><br></pre></td></tr></table></figure>

<pre><code>array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
      .........
       0.00000000e+00, 9.02126602e-01, 5.52653803e+00, 1.54785620e+01,
       2.06441456e+01, 3.43017827e+01, 5.21246770e+01, 7.97142975e+01,
       1.00081387e+02, 1.02608696e+02, 1.26673478e+02, 1.14405059e+02,
       8.98716430e+01, 6.32064973e+01, 3.98525769e+01, 2.14722761e+01,
       1.07222715e+01, 3.09714228e+00, 3.58912164e+00, 1.71614970e-01,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.median(X.var().values)         <span class="comment">#查看方差的中位数</span></span><br></pre></td></tr></table></figure>

<pre><code>1352.286703180131</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)        <span class="comment">#以中位数为阈值指标筛选特征，小于阈值的全部删除</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_fsvar.shape</span><br></pre></td></tr></table></figure>

<pre><code>(42000, 392)</code></pre><p><strong>当特征是二分类时，特征的取值就是伯努利随机变量。若特征是伯努利随机变量，假设p=0.8，即二分类特征中某种分类占到80%以上的时候删除特征</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_bvar = VarianceThreshold(<span class="number">.8</span> * (<span class="number">1</span> - <span class="number">.8</span>)).fit_transform(X)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_bvar.shape</span><br></pre></td></tr></table></figure>

<pre><code>(42000, 685)</code></pre><p><strong>特征经过筛选之后，真的能有利于模型吗？</strong> </p>
<h4 id="方差过滤对模型的影响"><a href="#方差过滤对模型的影响" class="headerlink" title="方差过滤对模型的影响"></a>方差过滤对模型的影响</h4><p>准备了KNN和随机森林分别在方差过滤前和方差过滤后运行的效果和运行时间的对比。KNN是K近邻算法中的分类算法，其原理非常简单，是利用每个样本到其他样本点的距离来判断每个样本点的相似度，然后对样本进行分类。<strong>KNN必须遍历每个特征和每个样本，因而特征越多，KNN的计算也就会越缓慢</strong>。</p>
<h5 id="KNN方差过滤前"><a href="#KNN方差过滤前" class="headerlink" title="KNN方差过滤前"></a>KNN方差过滤前</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#KNN vs 随机森林在不同方差过滤效果下的对比</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier <span class="keyword">as</span> RFC</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier <span class="keyword">as</span> KNN</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = data.iloc[:,<span class="number">1</span>:]</span><br><span class="line">y = data.iloc[:,<span class="number">0</span>]</span><br><span class="line">X_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%timeit       <span class="comment">#计算cell中代码运行时间</span></span><br><span class="line">cross_val_score(KNN(),X,y,cv=<span class="number">5</span>).mean()</span><br></pre></td></tr></table></figure>

<pre><code>35min 33s ± 12min 7s per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre><h5 id="KNN方差过滤后"><a href="#KNN方差过滤后" class="headerlink" title="KNN方差过滤后"></a>KNN方差过滤后</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%timeit</span><br><span class="line">cross_val_score(KNN(),X_fsvar,y,cv=<span class="number">5</span>).mean()</span><br></pre></td></tr></table></figure>

<h5 id="随机森林方差过滤前"><a href="#随机森林方差过滤前" class="headerlink" title="随机森林方差过滤前"></a>随机森林方差过滤前</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%timeit</span><br><span class="line">cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>),X,y,cv=<span class="number">5</span>).mean()</span><br></pre></td></tr></table></figure>

<h5 id="随机森林方差过滤后"><a href="#随机森林方差过滤后" class="headerlink" title="随机森林方差过滤后"></a>随机森林方差过滤后</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%timeit</span><br><span class="line">cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>),X_fsvar,y,cv=<span class="number">5</span>).mean()</span><br></pre></td></tr></table></figure>

<p>随机森林的准确率略逊于KNN，但运行时间却连KNN的1%都不到，只需要十几秒钟。其次，方差过滤后，随机森林的准确率也微弱上升，但运行时间却几乎是没什么变化，依然是11秒钟。</p>
<p><strong>为什么随机森林运行如此之快？为什么方差过滤对随机森林没很大的有影响？</strong>这是由于两种算法的原理中涉及到的计算量不同。最近邻算法KNN，单棵决策树，支持向量机SVM，神经网络，回归算法，都需要遍历特征或升维来进行运算，所以他们本身的运算量就很大，需要的时间就很长，因此方差过滤这样的特征选择对他们来说就尤为重要。但对于不需要遍历特征的算法，比如随机森林，它随机选取特征进行分枝，本身运算就非常快速，因此特征选择对它来说效果平平。这其实很容易理解，无论过滤法如何降低特征的数量，随机森林也只会选取固定数量的特征来建模；而最近邻算法就不同了，特征越少，距离计算的维度就越少，模型明显会随着特征的减少变得轻量。因此，过滤法的<strong>主要对象</strong>是：<strong>需要遍历特征或升维的算法们</strong>，而过滤法的<strong>主要目的</strong>是：<strong>在维持算法表现的前提下，帮助算法们降低计算成本</strong>。</p>
<h3 id="相关性过滤"><a href="#相关性过滤" class="headerlink" title="相关性过滤"></a>相关性过滤</h3><p>方差挑选完毕之后，我们就要考虑下一个问题：相关性了。我们希望选出与标签相关且有意义的特征，因为这样的特征能够为我们提供大量信息。如果特征与标签无关，那只会白白浪费我们的计算内存，可能还会给模型带来噪音。在sklearn当中，我们有三种常用的方法来评判特征与标签之间的相关性：卡方，F检验，互信息。</p>
<h4 id="卡方过滤"><a href="#卡方过滤" class="headerlink" title="卡方过滤"></a>卡方过滤</h4><p>卡方过滤是专门针对<strong>离散型</strong>标签（即分类问题）的相关性过滤。卡方检验类feature_selection.chi2计算每个<strong>非负特征</strong>和标签之间的卡方统计量，并依照卡方统计量由高到低为特征排名。再结合feature_selection.SelectKBest这个可以输入”评分标准“来选出前K个分数最高的特征的类，我们可以借此除去最可能独立于标签，与我们分类目的无关的特征。<br>另外，如果卡方检验检测到某个特征中所有的值都相同，会提示我们使用方差先进行方差过滤。并且，刚才我们已经验证过，当我们使用方差过滤筛选掉一半的特征后，模型的表现时提升的。因此在这里，我们使用threshold=中位数时完成的方差过滤的数据来做卡方检验（如果方差过滤后模型的表现反而降低了，那我们就不会使用方差过滤后的数据，而是使用原数据）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier <span class="keyword">as</span> RFC </span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#假设在这里我一直我需要300个特征</span></span><br><span class="line"><span class="comment">#SelectKBest的两个参数,第一个是依赖的统计量,此处为卡方"chi2",第二个参数表示,我知道我需要300个特征</span></span><br><span class="line">X_fschi = SelectKBest(chi2, k=<span class="number">300</span>).fit_transform(X_fsvar, y) </span><br><span class="line">X_fschi.shape</span><br></pre></td></tr></table></figure>

<pre><code>(42000, 300)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>),X_fschi,y,cv=<span class="number">5</span>).mean()  <span class="comment">#验证模型效果</span></span><br></pre></td></tr></table></figure>

<pre><code>0.9344761904761905</code></pre><p>可以看出，模型的效果降低了，这说明我们在设定k=300的时候删除了与模型相关且有效的特征，我们的K值设置得太小，要么我们需要调整K值，要么我们必须放弃相关性过滤。当然，如果模型的表现提升，则说明我们的相关性过滤是有效的，是过滤掉了模型的噪音的，这时候我们就保留相关性过滤的结果。</p>
<p><strong>选取超参数K</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">score = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">390</span>,<span class="number">200</span>,<span class="number">-10</span>):</span><br><span class="line">    X_fschi = SelectKBest(chi2, k=i).fit_transform(X_fsvar, y)</span><br><span class="line">    once = cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>),X_fschi,y,cv=<span class="number">5</span>).mean()</span><br><span class="line">    score.append(once)</span><br><span class="line">plt.plot(range(<span class="number">390</span>,<span class="number">200</span>,<span class="number">-10</span>),score)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20200802150933620.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTA0NDc1OA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>通过这条曲线，我们可以观察到，随着K值的不断增加，模型的表现不断上升，这说明，K越大越好，数据中所有的特征都是与标签相关的。但是运行这条曲线的时间同样也是非常地长，接下来我们就来介绍一种更好的选择k的方法：看p值选择k。<br><strong>P小于显著性水平,两者相关,大于显著性水平,两者相互独立</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chivalue, pvalues_chi = chi2(X_fsvar,y)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chivalue</span><br></pre></td></tr></table></figure>

<pre><code>array([ 945664.84392643, 1244766.05139164, 1554872.30384525,
       1834161.78305343, 1903618.94085294, 1845226.62427198,
       1602117.23307537,  708535.17489837,  974050.20513718,
      .....................
        203015.85080751,  255471.93956392,  389884.80778864,
        561308.92316732,  759511.94695328,  942402.80700557,
       1044698.95132913, 1009807.32615993,  844407.23356341,
        695110.21243546,  637789.62877943,  600582.89899187,
        519392.9652949 ,  399631.65341907, 1006027.89058975,
       1352052.90333271, 1647606.90721159, 1761733.4081397 ,
       1664096.76785043, 1396834.58681766, 1159784.3628775 ,
       1001178.01359166,  847886.28143964])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pvalues_chi   <span class="comment">#所以特征都小于显著性水平,说明特征与标签相关,如果将这些特征剔除,则会造成模型下降</span></span><br></pre></td></tr></table></figure>

<pre><code>array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
   ....................
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chivalue.shape   <span class="comment">#392个特征</span></span><br></pre></td></tr></table></figure>

<pre><code>(392,)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#k取多少？我们想要消除所有p值大于设定值，比如0.05或0.01的特征：</span></span><br><span class="line">k = chivalue.shape[<span class="number">0</span>] - (pvalues_chi &gt; <span class="number">0.05</span>).sum()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">k    <span class="comment">#保留的特征的数量</span></span><br></pre></td></tr></table></figure>

<pre><code>392</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#X_fschi = SelectKBest(chi2, k=填写具体的k).fit_transform(X_fsvar, y)</span></span><br><span class="line"><span class="comment">#cross_val_score(RFC(n_estimators=10,random_state=0),X_fschi,y,cv=5).mean()</span></span><br></pre></td></tr></table></figure>

<p>可以观察到，所有特征的p值都是0，这说明对于digit recognizor这个数据集来说，方差验证已经把所有和标签无关的特征都剔除了，或者这个数据集本身就不含与标签无关的特征。在这种情况下，舍弃任何一个特征，都会舍弃对模型有用的信息，而使模型表现下降，因此在我们对计算速度感到满意时，我们不需要使用相关性过滤来过滤我们的数据。</p>
<h4 id="F检验"><a href="#F检验" class="headerlink" title="F检验"></a>F检验</h4><p>F检验，又称ANOVA，方差齐性检验，是用来捕捉每个特征与标签之间的线性关系的过滤方法。它即可以做回归也可以做分类，因此包含feature_selection.f_classif（F检验分类）和feature_selection.f_regression（F检验回归）两个类。其中F检验分类用于标签是离散型变量的数据，而F检验回归用于标签是连续型变量的数据。<br><strong>F检验在数据服从正态分布时效果会非常稳定，因此如果使用F检验过滤，我们会先将数据转换成服从正态分布的方式</strong></p>
<p>F检验的本质是寻找两组数据之间的线性关系，其原假设是”数据不存在显著的线性关系“。它返回F值和p值两个统计量。<strong>和卡方过滤一样，我们希望选取p值小于0.05或0.01的特征，这些特征与标签时显著线性相关的，而p值大于0.05或0.01的特征则被我们认为是和标签没有显著线性关系的特征，应该被删除</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_classif</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F, pvalues_f = f_classif(X_fsvar,y)    <span class="comment">#特征矩阵和标签</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F</span><br></pre></td></tr></table></figure>

<pre><code>array([ 618.65383492,  846.18897012, 1115.40617051, 1362.3677305 ,
       1452.03355369, 1381.09095571, 1138.26505266,  464.29616121,
        660.00977785,  849.66393412, 1004.7450309 , 1124.76177588,
   ................
        819.19801306,  656.55547718,  510.87851723,  445.09613969,
        401.25608847,  333.48574029,  243.88699402,  645.9545719 ,
        920.3259526 , 1196.07900013, 1308.12260763, 1218.37705687,
        996.41501921,  792.59409228,  663.47516843,  550.14745143])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pvalues_f</span><br></pre></td></tr></table></figure>

<pre><code>array([0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,
       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,
      .........
       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,
       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,
       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,
       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,
       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">k = F.shape[<span class="number">0</span>] - (pvalues_f &gt; <span class="number">0.05</span>).sum()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">k</span><br></pre></td></tr></table></figure>

<pre><code>392</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_fsF = SelectKBest(f_classif, k=<span class="number">392</span>).fit_transform(X_fsvar, y)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>),X_fsF,y,cv=<span class="number">5</span>).mean()</span><br></pre></td></tr></table></figure>

<pre><code>0.9390476190476191</code></pre><p>得到的结论和我们用卡方过滤得到的结论一模一样：没有任何特征的p值大于0.01，所有的特征都是和标签相关<br>的，因此我们不需要相关性过滤。</p>
<h4 id="互信息法"><a href="#互信息法" class="headerlink" title="互信息法"></a>互信息法</h4><p>互信息法是用来捕捉每个特征与标签之间的任意关系（包括线性和非线性关系）的过滤方法。和F检验相似，它既可以做回归也可以做分类，并且包含两个类feature_selection.mutual_info_classif（互信息分类）和feature_selection.mutual_info_regression（互信息回归）。这两个类的用法和参数都和F检验一模一样，不过互信息法比F检验更加强大，F检验只能够找出线性关系，而互信息法可以找出任意关系。        </p>
<p>互信息法不返回p值或F值类似的统计量，它返回“每个特征与目标之间的互信息量的估计”，这个估计量在[0,1]之间取值，为0则表示两个变量独立，为1则表示两个变量完全相关。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> mutual_info_classif <span class="keyword">as</span> MIC</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result = MIC(X_fsvar,y)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result</span><br></pre></td></tr></table></figure>

<pre><code>array([0.05952806, 0.0895246 , 0.10150996, 0.11530121, 0.11525824,
       0.10318212, 0.08434178, 0.05440837, 0.07626707, 0.10205044,
       0.11799519, 0.15126027, 0.16091858, 0.16200295, 0.15825809,
       0.13055517, 0.08940818, 0.06768341, 0.0434172 , 0.02157236,
     ........
       0.09711366, 0.06874942, 0.04777689, 0.03643552, 0.01451625,
       0.03067249, 0.04974991, 0.05900978, 0.08409846, 0.10871554,
       0.12711551, 0.12253687, 0.1024499 , 0.08731526, 0.08065417,
       0.06260022, 0.04878859, 0.02961495, 0.06210424, 0.07927861,
       0.10196768, 0.11595027, 0.10862818, 0.10749328, 0.08828483,
       0.07072955, 0.05641889])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(result&gt;<span class="number">0</span>).sum()   <span class="comment">#392个全部大于零</span></span><br></pre></td></tr></table></figure>

<pre><code>392</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">k = result.shape[<span class="number">0</span>] - sum(result &lt;= <span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#X_fsmic = SelectKBest(MIC, k=填写具体的k).fit_transform(X_fsvar, y)</span></span><br><span class="line"><span class="comment">#cross_val_score(RFC(n_estimators=10,random_state=0),X_fsmic,y,cv=5).mean()</span></span><br></pre></td></tr></table></figure>

<h3 id="Embedded嵌入法"><a href="#Embedded嵌入法" class="headerlink" title="Embedded嵌入法"></a>Embedded嵌入法</h3><p>嵌入法是一种让算法自己决定使用哪些特征的方法，即特征选择和算法训练同时进行。在使用嵌入法时，我们先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小选择特征。这些权值系数往往代表了特征对于模型的某种贡献或某种重要性，比如决策树和树的集成模型中的feature_importances_属性，可以列出各个特征对树的建立的贡献，我们就可以基于这种贡献的评估，找出对模型建立最有用的特征。因此相比于过滤法，嵌入法的结果会更加精确到模型的效用本身，对于提高模型效力有更好的效果。并且，由于考虑特征对模型的贡献，因此无关的特征（需要相关性过滤的特征）和无区分度的特征（需要方差过滤的特征）都会因为缺乏对模型的贡献而被删除掉，可谓是过滤法的进化版。    </p>
<p>过滤法中使用的统计量可以使用统计知识和常识来查找范围（如p值应当低于显著性水平0.05），而<strong>嵌入法中使用的权值系数却没有这样的范围可找——我们可以说，权值系数为0的特征对模型丝毫没有作用，但当大量特征都对模型有贡献且贡献不一时，我们就很难去界定一个有效的临界值</strong>。   </p>
<p>另外，<strong>嵌入法引入了算法来挑选特征，因此其计算速度也会和应用的算法有很大的关系。如果采用计算量很大，计算缓慢的算法，嵌入法本身也会非常耗时耗力。并且，在选择完毕之后，我们还是需要自己来评估模型。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier <span class="keyword">as</span> RFC</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RFC_ = RFC(n_estimators =<span class="number">10</span>,random_state=<span class="number">0</span>) <span class="comment">#随机森林实例化</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_embedded = SelectFromModel(RFC_,threshold=<span class="number">0.005</span>).fit_transform(X,y) <span class="comment">#SelectFromModel实例化，使用X，看看在不过滤的情况</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在这里我只想取出来有限的特征。0.005这个阈值对于有780个特征的数据来说，是非常高的阈值，因为平均每个特征只能够分到大约0.001的feature_importances_</span></span><br><span class="line">X_embedded.shape</span><br></pre></td></tr></table></figure>

<pre><code>(42000, 47)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RFC_.fit(X,y).feature_importances_</span><br></pre></td></tr></table></figure>

<pre><code>array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
    .........
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">threshold = np.linspace(<span class="number">0</span>,(RFC_.fit(X,y).feature_importances_).max(),<span class="number">20</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">threshold</span><br></pre></td></tr></table></figure>

<pre><code>array([0.        , 0.00067177, 0.00134354, 0.00201531, 0.00268707,
       0.00335884, 0.00403061, 0.00470238, 0.00537415, 0.00604592,
       0.00671769, 0.00738945, 0.00806122, 0.00873299, 0.00940476,
       0.01007653, 0.0107483 , 0.01142007, 0.01209183, 0.0127636 ])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">score = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> threshold:</span><br><span class="line">    X_embedded = SelectFromModel(RFC_,threshold=i).fit_transform(X,y)</span><br><span class="line">    once = cross_val_score(RFC_,X_embedded,y,cv=<span class="number">5</span>).mean()</span><br><span class="line">    score.append(once)</span><br><span class="line"></span><br><span class="line">plt.plot(threshold,score)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20200802151015843.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTA0NDc1OA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="Wrapper包装法"><a href="#Wrapper包装法" class="headerlink" title="Wrapper包装法"></a>Wrapper包装法</h3><p>使用一个目标函数作为黑盒来帮助我们选取特征，而不是自己输入某个评估指标或统计量的阈值。包装法在初始特征集上训练评估器，并且通过coef_属性或通过feature_importances_属性获得每个特征的重要性。然后，从当前的一组特征中修剪最不重要的特征。在修剪的集合上递归地重复该过程，直到最终到达所需数量的要选择的特征。区别于过滤法和嵌入法的一次训练解决所有问题，包装法要使用特征子集进行多次训练，因此它所需要的计算成本是最高的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RFC_ = RFC(n_estimators =<span class="number">10</span>,random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">selector = RFE(RFC_, n_features_to_select=<span class="number">340</span>, step=<span class="number">50</span>).fit(X, y)    <span class="comment">#n_featrue_to_select是选择多少个特征，step是每次迭代删掉多少特征</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">selector.support_.sum()     <span class="comment">#返回所有的特征的是否最后被选中的布尔矩阵的加和</span></span><br></pre></td></tr></table></figure>

<pre><code>340</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">selector.ranking_       <span class="comment"># 返回特征的按数次迭代中综合重要性的排名</span></span><br></pre></td></tr></table></figure>

<pre><code>array([10,  9,  8,  7,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,
        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  6,  6,
        5,  6,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  6,  7,  7,
     .......
        2,  2,  3,  2,  2,  4,  4,  5,  5,  8,  8,  8,  7,  7,  7,  7,  7,
        7,  7,  5,  5,  4,  5,  4,  3,  3,  3,  4,  3,  3,  4,  3,  4,  5,
        5,  6,  7,  7,  7,  6,  7,  8,  8,  8,  9,  9,  9,  9,  6,  8,  8,
        8,  7,  8,  8,  8,  7,  8,  8,  8,  8,  8,  7,  8,  8,  8,  8,  9,
       10,  7])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_wrapper = selector.transform(X)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_val_score(RFC_,X_wrapper,y,cv=<span class="number">5</span>).mean()</span><br></pre></td></tr></table></figure>

<pre><code>0.9379761904761905</code></pre><p><strong>画学习曲线来选择特征数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">score = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">751</span>,<span class="number">50</span>):</span><br><span class="line">    X_wrapper = RFE(RFC_,n_features_to_select=i, step=<span class="number">50</span>).fit_transform(X,y)</span><br><span class="line">    once = cross_val_score(RFC_,X_wrapper,y,cv=<span class="number">5</span>).mean()</span><br><span class="line">    score.append(once)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">751</span>,<span class="number">50</span>),score)</span><br><span class="line">plt.xticks(range(<span class="number">1</span>,<span class="number">751</span>,<span class="number">50</span>))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20200802151030936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTA0NDc1OA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>在包装法下面，应用50个特征时，模型的表现就已经达到了90%以上，比嵌入法和过滤法都高效很多。我们可以放大图像，寻找模型变得非常稳定的点来画进一步的学习曲线（就像我们在嵌入法中做的那样）。如果我们此时追求的是最大化降低模型的运行时间，我们甚至可以直接选择50作为特征的数目，这是一个在缩减了94%的特征的基础上，还能保证模型表现在90%以上的特征组合，不可谓不高效。</p>
<p>同时，我们提到过，在特征数目相同时，包装法能够在效果上匹敌嵌入法。试试看如果我们也使用340作为特征数目，运行一下，可以感受一下包装法和嵌入法哪一个的速度更加快。由于包装法效果和嵌入法相差不多，在更小的范围内使用学习曲线，我们也可以将包装法的效果调得很好，大家可以去试试看。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/数据处理/" rel="tag"># 数据处理</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/07/28/实战：随机森林在乳腺癌数据上的调参/" rel="next" title="实战：随机森林在乳腺癌数据上的调参">
                <i class="fa fa-chevron-left"></i> 实战：随机森林在乳腺癌数据上的调参
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/08/03/RNA-seq结果图分析/" rel="prev" title="RNA-seq结果图分析">
                RNA-seq结果图分析 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Chen kun">
            
              <p class="site-author-name" itemprop="name">Chen kun</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">25</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">14</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#数据预处理"><span class="nav-number">1.</span> <span class="nav-text">数据预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#实现归一化"><span class="nav-number">1.1.</span> <span class="nav-text">实现归一化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#标准化"><span class="nav-number">1.2.</span> <span class="nav-text">标准化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#处理缺失值"><span class="nav-number">1.3.</span> <span class="nav-text">处理缺失值</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#填补年龄"><span class="nav-number">1.3.1.</span> <span class="nav-text">填补年龄</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用众数填补Embarked"><span class="nav-number">1.3.2.</span> <span class="nav-text">使用众数填补Embarked</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用Pandas和Numpy进行填补"><span class="nav-number">1.3.3.</span> <span class="nav-text">用Pandas和Numpy进行填补</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#处理分类型特征：编码和哑变量"><span class="nav-number">1.4.</span> <span class="nav-text">处理分类型特征：编码和哑变量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#preprocessing-LabelEncoder：标签专用，能够将分类转换为分类数值"><span class="nav-number">1.4.1.</span> <span class="nav-text">preprocessing.LabelEncoder：标签专用，能够将分类转换为分类数值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#preprocessing-OrdinalEncoder：特征专用，能够将分类特征转换为分类数值"><span class="nav-number">1.4.2.</span> <span class="nav-text">preprocessing.OrdinalEncoder：特征专用，能够将分类特征转换为分类数值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#preprocessing-OneHotEncoder：独热编码，创建哑变量"><span class="nav-number">1.4.3.</span> <span class="nav-text">preprocessing.OneHotEncoder：独热编码，创建哑变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#处理连续型特征：二值化与分段"><span class="nav-number">1.4.4.</span> <span class="nav-text">处理连续型特征：二值化与分段</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#sklearn-preprocessing-Binarizer"><span class="nav-number">1.4.4.1.</span> <span class="nav-text">sklearn.preprocessing.Binarizer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#preprocessing-KBinsDiscretizer"><span class="nav-number">1.4.4.2.</span> <span class="nav-text">preprocessing.KBinsDiscretizer</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#特征选择"><span class="nav-number">2.</span> <span class="nav-text">特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#特征工程的第一步是：理解业务"><span class="nav-number">2.1.</span> <span class="nav-text">特征工程的第一步是：理解业务</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Filter过滤法"><span class="nav-number">2.1.1.</span> <span class="nav-text">Filter过滤法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#方差过滤：VarianceThreshold"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">方差过滤：VarianceThreshold</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#方差过滤对模型的影响"><span class="nav-number">2.1.1.2.</span> <span class="nav-text">方差过滤对模型的影响</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#KNN方差过滤前"><span class="nav-number">2.1.1.2.1.</span> <span class="nav-text">KNN方差过滤前</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#KNN方差过滤后"><span class="nav-number">2.1.1.2.2.</span> <span class="nav-text">KNN方差过滤后</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#随机森林方差过滤前"><span class="nav-number">2.1.1.2.3.</span> <span class="nav-text">随机森林方差过滤前</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#随机森林方差过滤后"><span class="nav-number">2.1.1.2.4.</span> <span class="nav-text">随机森林方差过滤后</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#相关性过滤"><span class="nav-number">2.1.2.</span> <span class="nav-text">相关性过滤</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#卡方过滤"><span class="nav-number">2.1.2.1.</span> <span class="nav-text">卡方过滤</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#F检验"><span class="nav-number">2.1.2.2.</span> <span class="nav-text">F检验</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#互信息法"><span class="nav-number">2.1.2.3.</span> <span class="nav-text">互信息法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Embedded嵌入法"><span class="nav-number">2.1.3.</span> <span class="nav-text">Embedded嵌入法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Wrapper包装法"><span class="nav-number">2.1.4.</span> <span class="nav-text">Wrapper包装法</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chen kun</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Pisces</a> v6.0.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


















  
  









  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_lines.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.0.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.0.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.0.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.0.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.3"></script>



  



	





  





  










  





  

  

  

  

  
  

  

  

  

  

</body>
</html>
